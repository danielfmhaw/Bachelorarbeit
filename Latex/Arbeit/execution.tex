%! Author = danielmendes
%! Date = 11.12.24
\chapter{Projektdurchführung}\label{ch:projektdurchfuhrung}

\section{GitHub Action}\label{sec:github-action}

Im Laufe der Bachelorarbeit sind immer mehr unterschiedliche Projekte dazugekommen, die alle das Orchestrator-Skript benutzen.
Dadurch sind immer mehr Fallunterscheidungen in diesem Skript erforderlich geworden, und man hat schnell den Überblick verloren, wenn man Änderungen vorgenommen hat.
Um zu überprüfen, ob diese Änderungen negative Nebeneffekte haben, mussten jedes Mal alle Skripte nacheinander ausgeführt werden, was nicht nur zeitintensiv war, sondern auch hohe Lasten für den lokalen Rechner bedeutete.
Eine Möglichkeit wäre es gewesen, die Skripte parallel durchlaufen zu lassen, um Zeit zu sparen, aber damit wäre das Lastenproblem nicht gelöst worden.
Eine deutlich bessere Variante ist die Auslagerung in eine Pipeline.
In meinem Fall habe ich mich für GitHub Actions entschieden.
Vereinfacht gesagt sollen in der GitHub Action alle Skripte parallel ausgeführt und am Ende alle Output-Dateien in einen Ordner zusammen als GitHub Artifact hochgeladen werden.
Anschließend kann man die Zip-Datei herunterladen und überprüfen, ob alle Ergebnisse noch mit der Erwartung übereinstimmen.
Wenn fehlerhafte Änderungen hochgeladen wurden, scheitert der Workflow-Run direkt, und man hat einen guten Überblick über alle Projekte.

Damit dies funktioniert, muss eine YAML-Datei im Ordner \texttt{.github/workflows/} erstellt werden.
Anschließend bekommt der Workflow einen Namen und man kann definieren, wann er getriggert werden soll.
In meinem Fall, wenn sich etwas im \texttt{./github}-Ordner verändert, im \texttt{Projects/}-Ordner, in dem sich alle Lua-Skripte befinden, oder wenn sich das Orchestrator-Skript und die in diesem Skript benutzten Python-Dateien ändern.
Als Nächstes muss man eine JSON-Datei mit den Informationen zu den exportierten Variablen und den verwendeten Skripten befüllen und den einzelnen Projekten einen Namen geben.
Diesen Namen muss man auch in der Matrix definieren, damit die Informationen parallel aus der JSON-Datei entnommen und die Befehle ausgeführt werden können.

\lstinputlisting[
    language=Json,
    caption=JSON mit Konfiguration der Script,
    label={lst:script_configuration},
    style=custom_daniel,
]{Scripts/Execution/01_Pattern.json}

Anschließend muss man wie schon bei den Skripts davor die Umgebungsvariablen in der YAML – Datei definieren, wenn es sich um vertrauliche Informationen handeln sollte, bietet es sich an GitHub Secrets zu benutzen.
Danach beginnen erst die eigentlichen Schritte, die in dem Workflow ausgeführt werden.
Zunächst muss man das Repository auschecken und die passende Konfiguration aus der JSON – Datei laden, die dem Testtypen entspricht aus der Matrix.
Anschließend muss man nur noch die Abhängigkeiten installieren, die von den Skripten benötigt werden.
Dazu gehören unter anderem Sysbench, Pandas und Matplotlib.
Danach muss man den MySQL - Container mit den korrekten Umgebungsvariablen starten und als Nächstes wird das Orchestrator - Skript ausgeführt.
Dieses Skript erstellt, wie schon erklärt, die Graphen, CSV - und Logdateien in einem Output Ordner.
Um das Ganze aufzuräumen, kann man dem MySQL Container wieder stoppen und als letzten Schritt muss man nur noch den Output Ordner als Artifact hochladen.

\lstinputlisting[
    language=yaml,
    caption=Ausschnitt aus der Workflow - Datei,
    label={lst:workflow_yaml},
    style=custom_daniel,
    basicstyle=\ttfamily\scriptsize,
]{Scripts/Execution/02_Workflow.yaml}

Die eben beschriebene YAML - Datei reicht aus, damit alle angegebenen Skripten in dem JSON ausgeführt und die Output Dateien alle korrekt in einem Ordner als ZIP-Datei hochgeladen werden.
Es bieten sich aber auch Alternativen an, die zu einer Optimierung des Workflows führen.

Zum einen kann man die zu installierenden Abhängigkeiten mithilfe des GitHub Caches (\cite{github_cache_doku}) speichern.
Dies bietet sich besonders an, da sich die Abhängigkeiten über die Workflows hinweg nur selten ändern.
Falls sich doch etwas ändert, kann man beispielsweise die \texttt{require\allowbreak ments.txt}-Datei anpassen.
Dadurch werden einmalig alle Abhängigkeiten neu installiert und anschließend im Cache abgelegt.
Falls sich bis zum nächsten Workflow keine Änderungen an den Abhängigkeiten ergeben, wird der Cache automatisch genutzt. 
Der Zeitgewinn in unserem Beispiel ist jedoch nur gering und beträgt nur wenige Sekunden pro Workflow.

\lstinputlisting[
    language=yaml,
    caption=Speichern der Abhängigkeiten im Cache,
    label={lst:cache_yaml},
    style=custom_daniel,
]{Scripts/Execution/03_Cache.yaml}

Deutlich mehr Zeit und Ressourcen kann man aber sparen, wenn man zwischen zwei unterschiedlichen Arten von Dateien unterscheidet.
Denn zum einen gibt es Dateien, die die Ergebnisse von allen Skripten beeinflussen.
Dazu gehören das Workflow - Skript und die JSON - Datei, aber auch das Orchestrator - Skript und die darin verwendeten Python - Skripte.
Die Ordner an sich, die in der JSON angegeben werden, die beeinflussen nur sich selbst und nicht die anderen Skripte.
Beispielweise, wenn ich in Projekt A die Anzahl an Zeilen ändere, die ausgeführt werden, dann ändert dies nichts an dem Ergebnis von Projekt B oder C.
In diesem Beispiel würde es sich anbieten, dass für Projekt A die Benchmarks neu durchgeführt werden, für Projekt B und C könnte hingegen jeweils der letzte erfolgreiche Output Ordner benutzt werden.
Als Endresultat könnten damit die neue Durchführung von Projekt A zusammen mit der alten Ausführung der Projekte B und C in einer ZIP – Datei hochgeladen werden.
Dadurch wird nur ein Drittel der eigentlichen Ressourcen verbraucht, wenn man davon ausgehen würde, dass alle 3 Projekte gleich viel Zeit benötigen würden.

Für die Implementierung dieser Optimierung muss zunächst die allgemeinen Skripte hashen und zusätzlich noch die Ordner mit den Lua - Skripten, die für das jeweilige Skript aus der JSON benötigt werden.
Diese beiden Hashes kann zusammen mit den Testtypen kombinieren, damit bekommt die folgende Struktur für den Namen:

\begin{lstlisting}[language=yaml,label={lst:hash_name},style=custom_daniel]
NAME="${{ matrix.test-type }}-${{ env.hash }}-${{ env.general_hash }}"
\end{lstlisting}

Nachdem wir unsere JSON geladen haben, machen wir nun nicht mehr direkt mit der Installation der Abhängigkeiten weiter, sondern davor hashen wir die unterschiedlichen Pfade und erstellen unseren Namen.
Wenn es keinen Ordner mit dem gleichen Namen gibt, dann machen wir weiter wie bisher.
Das einzige, was sich ändert, ist der Schritt vor dem Hochladen des gesamten Output-Ordners.
Der vom Testtypen erzeugte Ordner muss zusammen mit seinem Namen hochgeladen werden, damit er im nächsten Workflow heruntergeladen werden kann, sofern die Hashes unverändert bleiben.
Ist dies beim nächsten Workflow der Fall, dann muss der Ordner nur heruntergeladen werden und and die korrekte Adresse im Output Ordner verschoben werden.
Die Installation der Abhängigkeiten, das Starten des MySQL - Containers und das Ausführen des Orchestrator - Skripts hat man sich damit erspart.

Die letzte Frage lautet, wo die Ordner mit den berechneten Namen gespeichert und beim nächsten Run wieder heruntergeladen werden sollen.
Zum einen kann man Lösungen in GitHub selbst verwenden.
Zum einen würde sich eine GitHub Cache - Lösung wieder anbieten, aber tatsächlich sind GitHub Artifacts für das Sichern von Dateien besser geeignet (\cite{github_cache_doku}).
Eine andere interessante Lösung kann auch das Nutzen von expliziten Branches nur für die Sicherung der Dateien seien.
Das Problem ist hier, dass es manchmal durch bestimmtes Timing zu Problemen beim Pushen kommen kann, da zufällig ein anderer paralleler Workflow in der Zeit zwischen Rebase, Commit und Push den Code verändert hat, wodurch nach verhindertem Push erneut ein Rebase durchgeführt werden muss.
Außerdem muss man dafür der GitHub Action Schreibberichtigungen geben.
Des Weiteren eignen sich auch Cloud - Speicherlösungen sehr gut, um die Ordner zu speichern und wieder herunterzuladen.
Dazu gehören von Google Cloud Storage (GCS), AWS S3 oder MS Azure Storage, die sich zusammen mit GitHub Artifacts am besten eignen.